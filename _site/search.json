[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am an experienced data scientist and team leader based in New York City. I’m at my best using data to answer business questions and solve problems."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nFlatiron School\n\nDirector, Data Science & AnalyticsMar 2022 to Present\nManager, Data Science & AnalyticsOct 2020 to Mar 2022\nSenior Data ScientistAug 2019 to Oct 2020\n\nJPMorgan Chase & Co. | Data ScientistFeb 2018 to Aug 2019\nLuxottica Retail | Senior AnalystDec 2015 to Feb 2018"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nMiami University | Oxford, OH\n\nMaster of Science in Statistics2014 - 2016\nBachelor of Science in Mathematics & Statistics2011 - 2016"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "Skills",
    "text": "Skills\n\n\n    \n        \n            \n        \n            \n        \n            \n        \n            \n        \n            \n        \n        \n         \n        R, SQL\n    \n\n    \n        \n            \n        \n            \n        \n            \n        \n            \n        \n        \n            \n        \n         \n        Python, Airflow, AWS (MWAA, S3, Lambda, RDS)\n    \n\n    \n        \n            \n        \n            \n        \n            \n        \n        \n            \n        \n            \n        \n         \n        Bash, Git, Hive, Impala\n    \n\n    \n        \n            \n        \n            \n        \n        \n            \n        \n            \n        \n            \n        \n         \n        Django, JavaScript, HTML, CSS\n    \n\n\nNo matching items"
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "About",
    "section": "Hobbies",
    "text": "Hobbies\nJiu jitsu, cooking, traveling"
  },
  {
    "objectID": "gallery/index.html",
    "href": "gallery/index.html",
    "title": "Gallery",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Description\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          url\n        \n         \n          thumbnail\n        \n     \n  \n\n\n    \n    \n        \n            \n        \n        \n            \n                Birdle\n            \n            A bird version of the NY Times' Wordle. Six guesses to identify the bird of the day.\n        \n    \n    \n    \n        \n            \n        \n        \n            \n                Quantifying Rafael Nadal’s Dominance with French Open Data\n            \n            A blog post written for Flatiron School\n        \n    \n    \n    \n        \n            \n        \n        \n            \n                The Data on Barbie, Greta Gerwig, and Best Director Snubs at the Oscars\n            \n            A blog post written for Flatiron School\n        \n    \n    \n    \n        \n            \n        \n        \n            \n                Taylor Swift and Data Science: An Unlikely Duo\n            \n            A blog post written for Flatiron School\n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-05-21-webcam/index.html",
    "href": "posts/2019-05-21-webcam/index.html",
    "title": "Webcam Image Classification",
    "section": "",
    "text": "As I mentioned in my “Hello World” post, I’m fascinated with computer vision. I’m in the process of writing up a post about the image classification mobile app that I made to detect whether my iPhone camera is pointing at my dog or not. While trying to figure out how to host a live ML model within a browser, I learned how to call pre-trained Tensorflow models from the HTML &lt;script&gt; tag. This option is pretty straightforward, but required learning a bit of JavaScript. If I tried to do this strictly in R, I anticipated embedding a Shiny application with an upload photo UI, pre-loaded model, image display, etc.\nIn this post, I create a webcam image classifier using a pre-trained TensorFlow.js MobileNet model hosted on jsDelivr. MobileNets are relatively small neural networks that are optimized to work well on less powerful computers (e.g. cell phones, tablets). This increased efficiency comes with a tradeoff in accuracy. Depending on the exact architecture chosen, MobileNets tend to top out around 60-70% accuracy. This particular model can predict 1,000 different classes, mostly commonplace objects and animals. I’ve found that it gets confused when the background is non-monochromatic, so for best results try to position the object in front of something simple. Before diving into the code, I am by no means a front end developer, so forgive the weak UI and delay in classification and display of the results. Below is the final product, followed by the instructions to recreate it. Have fun!\n\nThe final product!\n\n  Capture video\n  Take screenshot\n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\nHow it’s made:\nThe first step is to load the necessary libraries:\n\nTensorFlow.js to run the model\nThe MobileNet model to classify the images\nD3.js for visualization\n\n\n&lt;!-- Load TensorFlow.js. This is required to use MobileNet. --&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.1\"&gt;&lt;/script&gt;\n&lt;!-- Load the MobileNet model. --&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.1\"&gt;&lt;/script&gt;\n&lt;!-- Load D3 for the visualization of predicted image classifications --&gt;\n&lt;script src=\"https://d3js.org/d3.v4.min.js\"&gt;&lt;/script&gt;\n\n\n\n\n\n\n\nI add some (bare minimum) CSS so that the UI is somewhat presentable.\n\n&lt;style&gt;\n.bar {\n    fill: #27A822;\n}\n.axis {\n    font-size: 10px;\n}\n.axis path,\n.axis line {\n    fill: none;\n    display: none;\n}\n.label {\n    font-size: 13px;\n}\n.column {\n  float: left;\n  width: 33.33%;\n}\n.row:after {\n  content: \"\";\n  display: table;\n  clear: both;\n}\n&lt;/style&gt;\n\n\n\n\n\nNext, I add a basic UI with a “Capture video” and “Take screenshot” button that will start the webcam and screenshot a single frame to initiate object classification, respectively.\n&lt;p align=\"center\"&gt;\n  &lt;button id=\"capture-button\" class=\"capture-button\"&gt;Capture video&lt;/button&gt;\n  &lt;button id=\"screenshot-button\" disabled&gt;Take screenshot&lt;/button&gt;\n&lt;/p&gt;\n&lt;br&gt;\n&lt;div class=\"row\"&gt;\n  &lt;div class=\"column\"&gt;\n    &lt;video class=\"videostream\" width=\"240\" height=\"180\" autoplay&gt;&lt;/video&gt;\n  &lt;/div&gt;\n  &lt;div class=\"column\"&gt;\n    &lt;img id=\"screenshot-img\" width=\"240\" height=\"180\"&gt;\n  &lt;/div&gt;\n  &lt;div class=\"column\"&gt;\n    &lt;div id=\"graphic\" align=\"center\" width=\"240\" height=\"180\"&gt;&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\nNow that the UI is all set up, you need to make it react to input. To start, I initialize several variables that will be used later by the webcam. I also set up the size and axes of the graph that will ultimately display the top 3 classes and corresponding probabilities that the MobileNet predicts for the object in the still screenshot.\n\n// Initialize webcam and image settings\nconst constraints = {\n  video: true\n};\nconst captureVideoButton = document.getElementById('capture-button');\nconst screenshotButton = document.getElementById('screenshot-button');\nconst img = document.getElementById('screenshot-img');\nconst video = document.querySelector('video');\nconst canvas = document.createElement('canvas');\n\n// Set the dimensions and margins of the graph\nvar margin = {top: 0, right: 0, bottom: 25, left: 110},\n    width = 240 - margin.left - margin.right,\n    height = 180 - margin.top - margin.bottom;\n\n// Create the graph with the dimensions set above\nvar svg = d3.select(\"#graphic\").append(\"svg\")\n        .attr(\"width\", width + margin.left + margin.right)\n        .attr(\"height\", height + margin.top + margin.bottom)\n      .append(\"g\")\n        .attr(\"transform\", \n              \"translate(\" + margin.left + \",\" + margin.top + \")\");\n\n// Set the range of y and x\nvar y = d3.scaleBand()\n          .range([height, 0])\n          .padding(0.1);\n\nvar x = d3.scaleLinear()\n          .range([0, width]);\n\n// Initialize the axes with the x-axis formatted as a percentage     \nvar xAxis = d3.axisBottom(x)\n              .ticks(4)\n              .tickFormat(d =&gt; Math.round(d*100) + \"%\");\nvar yAxis = d3.axisLeft(y);\n\n// Attach the settings for the axes\nsvg.append(\"g\")\n    .attr(\"class\", \"y axis\")\n    .call(yAxis);\nsvg.append(\"g\")\n    .attr(\"class\", \"x axis\");\n\nBecause I decided to put the videostream, screenshot, and graph all on the same row of the site, the y-axis labels tended to get cut off, especially when the predicted class is something like “cellular telephone, cellular phone, cellphone, cell, mobile phone”. This next function was taken almost directly from here with minor tweaks to work on a horizontal bar chart rather than a vertical one. It takes text and a fixed width and splits it over several lines.\n\nfunction wrap(text, width) {\n  text.each(function() {\n    var text = d3.select(this),\n        words = text.text().split(/\\s+/).reverse(),\n        word,\n        line = [],\n        lineNumber = 0,\n        lineHeight = 0.5, // ems\n        y = text.attr(\"y\"),\n        dy = parseFloat(text.attr(\"dy\")),\n        tspan = text.text(null).append(\"tspan\").attr(\"x\", -10).attr(\"y\", y).attr(\"dy\", dy + \"em\")\n    while (word = words.pop()) {\n      line.push(word)\n      tspan.text(line.join(\" \"))\n      if (tspan.node().getComputedTextLength() &gt; width) {\n        line.pop()\n        tspan.text(line.join(\" \"))\n        line = [word]\n        tspan = text.append(\"tspan\")\n                        .attr(\"x\", -10)\n                        .attr(\"y\", y)\n                        .attr(\"dy\", `${++lineNumber * lineHeight + dy}em`)\n                        .text(word)\n      }\n    }\n  })\n}\n\nThe last function related to the D3.js graph is below. It updates the existing graph with the data it receives anytime it is called. It’s all pretty self-explanatory, so see the comments within the function.\n\nfunction update(data) {\n    \n    // Sort bars based on value\n    data = data.sort(function (a, b) { \n        return d3.ascending(a.probability, b.probability);\n    });\n    \n    // Format the data\n    data.forEach(function(d) {\n      d.probability = +d.probability;\n    });\n  \n    // Scale the range of the data in the domains\n    x.domain([0, d3.max(data, function(d){ return d.probability; })])\n    y.domain(data.map(function(d) { return d.className; }));\n  \n    // Remove any existing bars on the graph\n    var bars = svg.selectAll(\".bar\")\n        .remove()\n        .exit()\n        .data(data)\n    \n    // Add new bars using the new data\n    bars.enter()\n        .append(\"rect\")\n        .attr(\"class\", \"bar\")\n        .attr(\"width\", function(d) {return x(d.probability); } )\n        .attr(\"y\", function(d) { return y(d.className); })\n        .attr(\"height\", y.bandwidth());\n  \n    // Add the x Axis\n    svg.select(\".x\")\n        .attr(\"transform\", \"translate(0,\" + height + \")\")\n        .call(d3.axisBottom(x)\n                .ticks(4)\n                .tickFormat(d =&gt; Math.round(d*100) + \"%\"));\n  \n    // Add the y Axis\n    svg.select(\".y\")\n        .call(d3.axisLeft(y))\n      .selectAll(\".tick text\")\n        .call(wrap, 100);\n}\n\n\nSo far, I’ve set up the UI and the code to create the graph, but have not set up either the webcam or screenshot mechanism. In the next code chunk, I capture the video stream on button click using the getUserMedia() API. HTML5rocks.com has a very helpful article on video and audio capturing, if you’re interested in learning more. Next, I update the canvas element acting as the placeholder for the screenshot. That screenshot image then gets fed into the MobileNet model and the resulting predictions are passed in as the data to the update() function I wrote above.\n\n// When the video button is clicked, open the video webcam and begin streaming\ncaptureVideoButton.onclick = function() {\n  navigator.mediaDevices.getUserMedia(constraints).\n    then(handleSuccess).catch(handleError);\n};\n\nfunction handleSuccess(stream) {\n  screenshotButton.disabled = false;\n  video.srcObject = stream;\n};\n\n// When the screenshot button or the video itself is clicked,\n// grab a still image of the stream and replace the blank canvas\nscreenshotButton.onclick = video.onclick = function() {\n  \n  canvas.width = video.videoWidth;\n  canvas.height = video.videoHeight;\n  canvas.getContext('2d').drawImage(video, 0, 0);\n  // Other browsers will fall back to image/png\n  img.src = canvas.toDataURL('image/webp');\n  \n  const img_tmp = document.getElementById('screenshot-img');\n  \n  // Load the model\n  mobilenet.load().then(model =&gt; {\n    \n    // Classify the image\n    model.classify(img_tmp).then(predictions =&gt; {\n      \n      // Update the bar chart\n      update(predictions);\n            \n    });\n  });\n};"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Inaugural Mastery Quest - Summer Olympics\n\n\n\n\n\nLearn more on Bletchley.org!\n\n\n\n\n\nJul 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWebcam Image Classification\n\n\n\n\n\n\nJavascript\n\n\n\n\n\n\n\n\n\nMay 21, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nRaspberry Pi Dog Monitor\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nMay 13, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping NHL Data with RSelenium, Part 1\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nHello World\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 7, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-05-13-raspberry-pi/index.html",
    "href": "posts/2019-05-13-raspberry-pi/index.html",
    "title": "Raspberry Pi Dog Monitor",
    "section": "",
    "text": "Crate-training a puppy can be tough and a total surprise when you come home to find that they’ve torn up their bed, had an accident, or worse, broken free and vandalized your apartment. My dog, Harper, has done all three, but has since learned to love her cozy den. As I teased in my “Hello World” post, I built a dog monitor using a Raspberry Pi to see what she was up to while I was gone. She didn’t break out this time, but she was a bit mischievous.\nThe hardware requirements for this project are a Raspberry Pi and camera. If you need help setting up the camera, PyImageSearch is a great resource and can help you set it up. If you’ve never read it before, I highly recommend the blog. It has tons of posts and resources for learning OpenCV and applying deep learning computer vision techniques. It’s where I caught the computer vision bug and found inspiration for several projects.\nI still haven’t figured out how to edit mp4/mov files into timelapse videos, so instead I took still photos at set time intervals and stitched them together into a gif with a brief delay. To do this, I used ImageMagick. On the Raspberry Pi, it was as simple as:\nsudo apt-get update\nsudo apt-get install imagemagick\nOnce I set up my RPi and camera, I switched over to Python. Below are the necessary packages for this project. Something worth calling out is that with the smtplib package, Gmail can get mad about “less secure apps” trying to sign in. Although not a good long-term solution, the workaround for this is to go into your Google security settings and allowing these apps.\n# General use packages\nimport os\nimport time\nimport getpass\n# Packages needed for RPi Camera\nfrom picamera.array import PiRGBArray\nfrom picamera import PiCamera\n# Packages for emailing the video captured\nimport smtplib\nfrom email import encoders\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nfrom email.mime.base import MIMEBase\nNext, I set the email address from which I wanted to send the timelapse gif as well as the list of addresses to which I wanted to send it. I also initialized the SMTP connection and authenticated in this step.\n# Set email address to send from,\n#   email address (list) to send to, and \n#   initialize SMTP connection to Gmail\nfromaddr = \"beebe.mitch@gmail.com\"\nemailTo = [\"beebe.mitch@gmail.com\"]\nserver = smtplib.SMTP('smtp.gmail.com', 587)\nserver.starttls()\nserver.login(fromaddr, getpass.getpass(\"Enter password: \"))\nNow that the email connection was set up, I cleared out any files associated with previous executions. I then initialized the RPi camera and begain taking still photos at 60 second intervals for 60 minutes * 4 hours, saving each photo in the same directory as image0000.jpg, image0001.jpg, image0002.jpg, and so on.\n# Delete old gifs\nif os.path.isfile(\"harperMonitor.gif\"):\n    os.remove(\"harperMonitor.gif\")\n\n# Delete old jpg images saved\nfilelist = [ f for f in os.listdir(\".\") if f.endswith(\".jpg\") ]\nfor f in filelist:\n    os.remove(f)\n\n# Initialize camera\ncamera = PiCamera()\ncamera.resolution = (1024, 768)\n\n# Take a still jpg picture every minute for 4 hours\n#   and save image with 4-digit suffix with image index\nfor i in range(60*4):\n    camera.capture('image{0:04d}.jpg'.format(i))\n    time.sleep(60)\nWhen that loop finished, I then executed the code below from Python via the command line. Convert is a command from ImageMagick that allows you to manipulate image formats. I told it to convert every file with the .jpg extension into a gif with a tenth of a second delay between each image and no loop.\n# Convert jpg to gif with 0.1 second delay\nos.system('convert -delay 10 -loop 0 image*.jpg harperMonitor.gif')\nNext, I initialized a MIMEMultipart message with proper to and from emails, subject, and body. Lastly, I opened the gif and attached it using the MIMEImage subclass. Now the email was ready to send and close the connection.\n# Initialize a multipart email (one with body, attachments, etc) \nmsg = MIMEMultipart()\n\n# Add From, To, Subject, and Body of email\nmsg['From'] = fromaddr\nmsg['To'] = \", \".join(toaddr)\nmsg['Subject'] = \"Harper Monitor for \" + time.strftime(\"%m/%d/%y\")\nbody = \"Here is your daily Harper video! Woof.\"\nmsg.attach(MIMEText(body, 'plain'))\n\n# Attach the gif to the email\nfp = open(\"harperMonitor.gif\",\"rb\")\nmsgImg = MIMEImage(fp.read())\nfp.close()\nmsg.attach(msgImg)\n\n# Send the email and close the connection\nserver.sendmail(fromaddr, toaddr, msg.as_string())\nserver.quit()\nThat’s it! It takes ~ 4 hours to run (duh), but you can alter the frequency and number of images taken depending on how long Harper is in her crate. Below is a cropped version of the end result that I received in my Gmail inbox. Someone got ornery while I was away! Thanks for reading."
  },
  {
    "objectID": "posts/2019-05-07-hello-world/index.html",
    "href": "posts/2019-05-07-hello-world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "This is my first post! This website is developed with Quarto and served via Netlify. I tried hosting this on GitHub Pages, but had too much difficulty customizing. Having used Blogdown, I’m making the transition to Quarto.\nSo far this is great. I can embed R code like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nOr include plots:\n\npie(\n  c(280, 60, 20),\n  c('Sky', 'Sunny side of pyramid', 'Shady side of pyramid'),\n  col = c('#0292D8', '#F7EA39', '#C4B632'),\n  init.angle = -50, border = NA\n)\n\n\n\n\n\n\n\n\n\nI don’t anticipate blogging a lot, but I hope to make this a platform to showcase some of my pet projects. Fair warning, a lot of my projects revolve on my dog, Harper, so I literally mean “pet”. Below are a few examples I hope to post about soon:\n\nRaspberry Pi timelapse video of Harper crate-training\nHarper/Not Harper image classification mobile app\nHarper autoencoder to generate new, “fake” Harper images\n\nThanks for reading!"
  },
  {
    "objectID": "posts/2019-05-07-hello-world/index.html#quarto",
    "href": "posts/2019-05-07-hello-world/index.html#quarto",
    "title": "Hello World",
    "section": "",
    "text": "This is my first post! This website is developed with Quarto and served via Netlify. I tried hosting this on GitHub Pages, but had too much difficulty customizing. Having used Blogdown, I’m making the transition to Quarto.\nSo far this is great. I can embed R code like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nOr include plots:\n\npie(\n  c(280, 60, 20),\n  c('Sky', 'Sunny side of pyramid', 'Shady side of pyramid'),\n  col = c('#0292D8', '#F7EA39', '#C4B632'),\n  init.angle = -50, border = NA\n)\n\n\n\n\n\n\n\n\n\nI don’t anticipate blogging a lot, but I hope to make this a platform to showcase some of my pet projects. Fair warning, a lot of my projects revolve on my dog, Harper, so I literally mean “pet”. Below are a few examples I hope to post about soon:\n\nRaspberry Pi timelapse video of Harper crate-training\nHarper/Not Harper image classification mobile app\nHarper autoencoder to generate new, “fake” Harper images\n\nThanks for reading!"
  },
  {
    "objectID": "posts/2019-05-08-nhlWebscraping/index.html",
    "href": "posts/2019-05-08-nhlWebscraping/index.html",
    "title": "Webscraping NHL Data with RSelenium, Part 1",
    "section": "",
    "text": "I love hockey. I also love data science. What’s better than merging the two and learn how to use RSelenium in the process?\nA while ago, I wrote some web-scraping code to find the NHL standings as of Novermber 1st (roughly one month into the season). I was doing this in order to determine if a strong start to the season was related to end-of-season success. Fortunately, the website dropyourgloves.com had convenient and uniform URLs to simply combine season and date strings to create a valid URL to scrape the corresponding HTML table. Unfortunately, the website no longer exists!\nAfter some Google-Fu, I was unable to find a replacement website, however, I found shrpsports.com, which has an easy-to-use dropdown UI, submit bottons, and resulting HTML table. This isn’t as easy as find and replace in the code, so I needed to load RSelenium for automating the browser to populate dropdowns and click submit.\nI found that the preferred way to run RSelenium is via a headless browser running in Docker. This was the first case I found myself needing either technology. I still have work to do to write more robust code, but I got this working, so I wanted to post about it.\nSet up Docker\n\nFirst, I downloaded Docker here\nTo start a headless browser, I ran docker run -d -p 4445:4444 selenium/standalone-chrome (the first time pulls the Docker image from DockerHub)\n\nConnecting to the browser\nThe R code below begins the webscraping journey.\n\n# Import packages\nlibrary(RSelenium)\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Start a docker container with Google Chrome on port 4444 on\n#   the server side inside the container and 4445 on my local machine\nsystem(\"docker run -d -p 4445:4444 selenium/standalone-chrome\")\n\n# Access the remote browser\nremDr &lt;- RSelenium::remoteDriver(remoteServerAddr = \"localhost\",\n                                 port = 4445L,\n                                 browserName = \"chrome\")\n\n# Initialize a browsing session\nremDr$open(silent = TRUE)\n\n# Navigate to the website to scrape\nremDr$navigate(\"http://www.shrpsports.com/nhl/stand.htm\") \n\n# Save a screenshot and display below\nremDr$screenshot(file = \"screenshot.png\")\n\n\nHere’s what the website looks like:\n\n\n\nScrape NHL Standings\nNow for the fun part. I wrote this function to fetch standings from the remote browser for any season and date or for season-end standings. There are a few pitfalls of this function regarding the NHL changing playoff format, conference assignments, etc. that will take quite a bit of elbow grease for a truly robust function, so I’ll save that for another time. I also noticed a few data consistency issues, but hey, it’s a free site from which I’m pulling data.\nFor the timebeing, this function simply returns the Eastern, Western, and, if pulling season-end standings, the Stanley Cup match-up. In a future post, I hope to try the analysis of “Does a strong start to the season predict not only a playoff berth, but also playoff success?” Having this function will allow for that to happen with far less repetitive programming.\n\ngetStandings &lt;- function(season, month, date) {\n  # Gets the NHL standings for any season on any date or final conference standings\n  #\n  # Args:\n  #   season: Four-character string representing the NHL season (year in which Stanley\n  #     Cup is played for the season, e.g. \"2018\" is for the 2017-18 season)\n  #   month (optional): Three-character month abbreviation\n  #   date (optional): Character representing day of the month (e.g. \"1\", \"12\", \"27\")\n  #\n  # Returns:\n  #   The NHL standings in a dataframe\n  \n  # Enter the URL for the browser\n  remDr$navigate(\"http://www.shrpsports.com/nhl/stand.htm\") \n  \n  # Save the homepage HTML to reuse in several \n  homepage &lt;- read_html(remDr$getPageSource()[[1]])\n  \n  # Get seasons from dropdown\n  valid_seasons &lt;- homepage %&gt;% \n    html_nodes(\"select[name='season']\") %&gt;% \n    html_children() %&gt;% \n    html_attr(\"value\")\n  \n  # Get months from dropdown\n  valid_mos &lt;- homepage %&gt;% \n    html_nodes(\"select[name='month']\") %&gt;% \n    html_children() %&gt;% \n    html_attr(\"value\")\n  \n  # Get days of month from dropdown\n  valid_dates &lt;- homepage %&gt;% \n    html_nodes(\"select[name='date']\") %&gt;% \n    html_children() %&gt;% \n    html_attr(\"value\")\n  \n  # Verify season input\n  if (!(season %in% valid_seasons)) stop(\"Invalid season\")\n  \n  # Determine if user wants final standings or standings as of a date\n  if (missing(month) | missing(date)) {\n    div_conf &lt;- \"latefincnf\"\n    month &lt;- \"\"\n    date &lt;- \"\"\n    message(\"Getting season-end standings...\")\n  } else {\n    if (!(month %in% valid_mos)) stop(\"Invalid month\")\n    if (!(date %in% valid_dates)) stop(\"Invalid date\")\n    div_conf &lt;- \"cnf\"\n    message(glue(\"Getting standings as of {month}-{date}...\"))\n  }\n  \n  # Select season input in dropdown\n  season &lt;- remDr$findElement(using = 'css selector', \n                              glue(\"select[name='season'] option[value='{season}']\"))\n  season$clickElement()\n  \n  # Select division/conference in dropdown\n  divcnf &lt;- remDr$findElement(using = 'css selector', \n                              glue(\"select[name='divcnf'] option[value='{div_conf}']\"))\n  divcnf$clickElement()\n  \n  # Select month in dropdown\n  month &lt;- remDr$findElement(using = 'css selector', \n                             glue(\"select[name='month'] option[value='{month}']\"))\n  month$clickElement()\n  \n  # Select day of month in dropdown\n  dom &lt;- remDr$findElement(using = 'css selector', \n                           glue(\"select[name='date'] option[value='{date}']\"))\n  dom$clickElement()\n  \n  # Click submit botton\n  submit &lt;- remDr$findElement(using = 'css selector', \"input[type='submit']\")\n  submit$clickElement()\n  \n  # NOT RUN: This will take a screenshot of the current remote browser\n  #   screen and display it in the RStudio viewer\n  # remDr$screenshot(display = TRUE) \n  \n  # Read the HTML table from resulting webpage\n  raw_table &lt;- read_html(remDr$getPageSource()[[1]]) %&gt;% \n    html_table(fill = TRUE) %&gt;% \n    .[[3]]\n  \n  # Names are stored in the second row, so rename the table accordingly\n  names(raw_table) &lt;- raw_table[2,]\n  \n  # Clean up column name holding the NHL team name and remove excess rows\n  raw_table &lt;- raw_table %&gt;% \n    rename(Team = \"\") %&gt;% \n    filter(Team != \"\") %&gt;% \n    rename_all(~str_replace_all(.,\"\\\\-\", \"\\\\_\"))\n  \n  # Index the rows of the table holding Conference subheadings\n  conf_idx &lt;- raw_table$Team %&gt;% \n    grep(\"conf\", ., ignore.case = TRUE)\n  \n  # Store all Eastern conference results in a dataframe\n  east &lt;- raw_table %&gt;% \n    slice(1:(conf_idx[2] - 1)) %&gt;% \n    filter(!str_detect(Team, regex(\"conf\", ignore_case = TRUE))) %&gt;% \n    mutate(place = row_number(),\n           Team = str_trim(str_replace(Team, \"\\\\*|\\\\d\", \"\")),\n           playoffs = if_else(place &lt;= 8, TRUE, FALSE))\n  \n  # Store all Western conference results in another dataframe\n  west &lt;- raw_table %&gt;% \n    slice(conf_idx[2]:n()) %&gt;% \n    filter(!str_detect(Team, regex(\"conf\", ignore_case = TRUE))) %&gt;% \n    mutate(place = row_number(),\n           Team = str_trim(str_replace(Team, \"\\\\*|\\\\d\", \"\")),\n           playoffs = if_else(place &lt;= 8, TRUE, FALSE))\n  \n  # If user wants final conference standings, also get Stanley Cup match\n  if (div_conf == \"latefincnf\") {\n    \n    sc_match &lt;- read_html(remDr$getPageSource()[[1]]) %&gt;%\n      html_table(fill = TRUE) %&gt;%\n      tail(1) %&gt;%\n      .[[1]] %&gt;% \n      filter(!str_detect(X1, regex(\"cup\", ignore_case = TRUE))) %&gt;%\n      mutate(X1 = str_remove_all(X1, \"[\\\\d\\\\-]\") %&gt;% \n               str_remove(\"\\\\w+$\") %&gt;% \n               str_trim()) %&gt;%\n      separate(X1, c(\"Winner\", \"Loser\"), \"  \")\n    \n  }\n  \n  return(\n    list(eastern = east,\n         western = west,\n         stanley_cup = if (exists(\"sc_match\")) sc_match else NA)\n  )\n  \n}\n\nBelow are sample results of running the function. Looks like it works for season-end standings!\n\n# Let's get the season-end Eastern standings for the 2016 season\ngetStandings(\"2016\")$eastern %&gt;% \n  kable() %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n                full_width = FALSE, font_size = 12)\n\nLet’s see how my Detroit Red Wings were doing on the day I was born…\n\ndrw &lt;- getStandings(\"1993\", \"Feb\", \"14\")$western %&gt;% \n  filter(Team == \"Detroit\")\n\ndrw %&gt;% \n  kable() %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n                full_width = FALSE, font_size = 12)\n\nA record of 31-21-7…not bad. Fun fact, Montreal won the cup that year.\nThat’s it for now, thanks for reading. Hopefully more to come on this."
  },
  {
    "objectID": "posts/2024-07-16-data-quest/index.html",
    "href": "posts/2024-07-16-data-quest/index.html",
    "title": "Inaugural Mastery Quest - Summer Olympics",
    "section": "",
    "text": "This week’s Data Science Mastery Quest, or Data Quest, was to find a dataset on the Summer Olympics and create a visualization, model, app, or other data-related output using the tool of your choice.\nBelow is my contribution followed by the how-to, for those interested."
  },
  {
    "objectID": "posts/2024-07-16-data-quest/index.html#how-i-made-it",
    "href": "posts/2024-07-16-data-quest/index.html#how-i-made-it",
    "title": "Inaugural Mastery Quest - Summer Olympics",
    "section": "How I Made It",
    "text": "How I Made It\nI started with the research question, what countries have been the most dominant all-time?\nTo start, I found this data from an old Kaggle contest. In order to manipulate and visualize it, I’m loading the tidyverse of packages and the emoji package for fun graphics.\n\nlibrary(tidyverse)\nlibrary(emoji)\nlibrary(ggrepel)\nlibrary(ggtext)\nlibrary(ggimage)\n\nolympics &lt;- readr::read_csv('athlete_events.csv') %&gt;% rename_with(tolower)\nnoc &lt;- readr::read_csv(\"noc_regions.csv\") %&gt;% rename_with(tolower)\n\nTaking a peek at the data, I can see it’s one row per athlete per year per event. I also see it has the Winter Olympics included. I’ll want to filter these out considering the Summer Olympics theme. I’ll also need to count only one medal per event so team sports like basketball and relays don’t skew the counts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nname\nsex\nage\nheight\nweight\nteam\nnoc\ngames\nyear\nseason\ncity\nsport\nevent\nmedal\n\n\n\n\n1\nA Dijiang\nM\n24\n180\n80\nChina\nCHN\n1992 Summer\n1992\nSummer\nBarcelona\nBasketball\nBasketball Men’s Basketball\nNA\n\n\n2\nA Lamusi\nM\n23\n170\n60\nChina\nCHN\n2012 Summer\n2012\nSummer\nLondon\nJudo\nJudo Men’s Extra-Lightweight\nNA\n\n\n3\nGunnar Nielsen Aaby\nM\n24\nNA\nNA\nDenmark\nDEN\n1920 Summer\n1920\nSummer\nAntwerpen\nFootball\nFootball Men’s Football\nNA\n\n\n4\nEdgar Lindenau Aabye\nM\n34\nNA\nNA\nDenmark/Sweden\nDEN\n1900 Summer\n1900\nSummer\nParis\nTug-Of-War\nTug-Of-War Men’s Tug-Of-War\nGold\n\n\n5\nChristine Jacoba Aaftink\nF\n21\n185\n82\nNetherlands\nNED\n1988 Winter\n1988\nWinter\nCalgary\nSpeed Skating\nSpeed Skating Women’s 500 metres\nNA\n\n\n5\nChristine Jacoba Aaftink\nF\n21\n185\n82\nNetherlands\nNED\n1988 Winter\n1988\nWinter\nCalgary\nSpeed Skating\nSpeed Skating Women’s 1,000 metres\nNA\n\n\n\n\n\nI’d like to visualize cumulative medal counts by country to see if there are stretches of dominance. To do this, I’ll need to aggregate the athlete-event data by country and year. I also need a reference table for the country flag emojis.\n\ncumulative_medals &lt;- \n  olympics %&gt;% \n  left_join(\n    noc,\n    by = \"noc\"\n  ) %&gt;% \n  filter(season == \"Summer\", !is.na(medal)) %&gt;% \n  # Rename UK and USA for emoji match later\n  mutate(\n    team = case_when(\n      noc == \"GBR\" ~ \"United Kingdom\",\n      noc == \"USA\" ~ \"United States\",\n      TRUE ~ region\n    )\n  ) %&gt;% \n  # Aggregate medals by country and year\n  group_by(team, year, medal) %&gt;% \n  summarise(n = n_distinct(event), .groups = \"drop\") %&gt;% \n  pivot_wider(names_from = medal, values_from = n, values_fill = 0) %&gt;% \n  mutate(Total = Gold + Silver + Bronze) %&gt;% \n  group_by(team) %&gt;% \n  arrange(team, year) %&gt;% \n  # Create a cumulative sum\n  mutate(across(Bronze:Total, ~ cumsum(replace_na(.x, 0))))\n\ncountry_flags &lt;- \n  emojis %&gt;% \n  filter(subgroup == \"country-flag\") %&gt;% \n  mutate(country = str_extract(name, \"(?&lt;=flag: ).*\"))\n\nWith these aggregate tables, I’m ready to plot. See inline comments for detail.\n\nby_country &lt;-\n  cumulative_medals %&gt;% \n  # Only show countries with at least 500 total medals\n  filter(max(Total) &gt;= 500) %&gt;% \n  left_join(country_flags, by = c(\"team\" = \"country\")) %&gt;% \n  group_by(team) %&gt;% \n  # Only label the last year with the flag emoji to avoid noise\n  mutate(label = if_else(year == max(year), emoji, NA_character_)) %&gt;% \n  ggplot(aes(x = year, y = Total)) + \n  # Highlight Russia/USSR in red during Cold War years\n  geom_line(\n    aes(\n      group = team,\n      color = if_else(\n        team == \"Russia\" & year &gt;= 1947 & year &lt;= 1991,\n        \"#FF3C28\",\n        \"grey60\"\n      )\n    )\n  ) +\n  scale_color_identity() +\n  geom_text_repel(\n    aes(label = label), \n    family = \"DIN Alternate\",\n    hjust = 0,\n    vjust = 0.5,\n    nudge_y = 15,\n    segment.size = 0.2,\n    xlim = c(2016, NA)\n  ) +\n  # Set ticks to years divisible by four to align with years in which Summer Games were held\n  scale_x_continuous(limits = c(1896, 2024), breaks = seq(1896, 2024, by = 16)) +\n  scale_y_continuous(\n    limits = c(0, 3000),\n    labels = scales::comma,\n    position = \"right\") +\n  labs(\n    title = \"&lt;b&gt;The Cold War: A Space Race or Foot Race?&lt;/b&gt;&lt;br&gt;The &lt;span style='color: #ff3c28'&gt;USSR&lt;/span&gt;&lt;sup&gt;1&lt;/sup&gt; wins over 1,000 medals -- the most of any country in this period\",\n    subtitle = \"Cumulative Summer Olympics medal count by country for countries with at least 500 medals\",\n    x = NULL,\n    y = NULL,\n    caption = \"Data source: Kaggle&lt;br&gt;&lt;sup&gt;1&lt;/sup&gt; USSR/Russia are combined as are Germany/East Germany for this analysis\") +\n  # Add Cold War annotation\n  geom_errorbarh(\n    inherit.aes = FALSE,\n    data = tibble(xmin = 1947, xmax = 1991, y = 2600),\n    aes(xmin = xmin, xmax = xmax, y = y),\n    height = 100\n  ) +\n  annotate(\n    \"text\", x = 1969, y = 2700, \n    label = \"The Cold War (1947-1991)\", \n    family = \"DIN Alternate\",\n    hjust = 0.5, vjust = 0\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(margin = margin(l = 100, b = 5), vjust = 0.5),\n    plot.subtitle = element_markdown(margin = margin(l = 100, b = 10)),\n    plot.caption = element_markdown(),\n    text = element_text(family = \"DIN Alternate\"),\n    axis.line = element_line(color = \"grey20\"),\n    strip.text = element_text(size = 48)\n    )  +\n  coord_cartesian(clip = 'off') +\n  # Add Olympic Rings logo\n  annotation_custom(\n    magick::image_read(\"olympic_rings.svg\") %&gt;% grid::rasterGrob(interpolate = TRUE), \n    x = 1890, xmax = 1914, y = 3300, ymax = 4100)\n\nggsave(by_country, file = \"by-country.svg\", width = 8, height = 4)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "{{< fa chart-line >}} Mitch Beebe",
    "section": "",
    "text": "Welcome to my personal site.\nFind me @mitchbeebe on most platforms or shoot me an email.\n\n\nPowered by Quarto"
  }
]