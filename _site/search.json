[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am an experienced data scientist and team leader based in New York City. I’m at my best using data to answer business questions and solve problems."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nFlatiron School\n\nDirector, Data Science & AnalyticsMar 2022 to Present\nManager, Data Science & AnalyticsOct 2020 to Mar 2022\nSenior Data ScientistAug 2019 to Oct 2020\n\nJPMorgan Chase & Co. | Data ScientistFeb 2018 to Aug 2019\nLuxottica Retail | Senior AnalystDec 2015 to Feb 2018"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nMiami University | Oxford, OH\n\nMaster of Science in Statistics2014 - 2016\nBachelor of Science in Mathematics & Statistics2011 - 2016"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "Skills",
    "text": "Skills\n\n\n    \n        \n            \n        \n            \n        \n            \n        \n            \n        \n            \n        \n        \n         \n        R, SQL\n    \n\n    \n        \n            \n        \n            \n        \n            \n        \n            \n        \n        \n            \n        \n         \n        Python, Airflow, AWS: S3, Lambda, SageMaker, Glue, API Gateway, CloudWatch\n    \n\n    \n        \n            \n        \n            \n        \n            \n        \n        \n            \n        \n            \n        \n         \n        Bash, Git, Hive, Impala\n    \n\n    \n        \n            \n        \n            \n        \n        \n            \n        \n            \n        \n            \n        \n         \n        Django, JavaScript, HTML, CSS\n    \n\n\nNo matching items"
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "About",
    "section": "Hobbies",
    "text": "Hobbies\nJiu jitsu, cooking, traveling"
  },
  {
    "objectID": "gallery/taylor-swift/analysis.html",
    "href": "gallery/taylor-swift/analysis.html",
    "title": "Taylor Swift and Data Science: An Unlikely Duo",
    "section": "",
    "text": "This article was originally posted to Flatiron School’s Blog, but has since been moved\nData is everywhere, but one thing that might be more ubiquitous than data is Taylor Swift. The recent article “Taylor’s Towering Year”—authored by Posit (formerly RStudio)—illustrates several ways in which the two are not mutually exclusive by showing the data behind her record-breaking Eras Tour. In the article, they break down the tour’s staggering ticket sales, profound effect on worldwide economies, and boost in popularity for Taylor’s opening acts. Let’s discuss how Posit accomplished this and show you a concert tour visualization of our own."
  },
  {
    "objectID": "gallery/taylor-swift/analysis.html#quarto",
    "href": "gallery/taylor-swift/analysis.html#quarto",
    "title": "Taylor Swift and Data Science: An Unlikely Duo",
    "section": "Quarto",
    "text": "Quarto\nFirst released in early 2021, Quarto, the tool behind the Eras Tour article, is an open-source publishing system designed to weave prose and code output into dynamic documents, presentations, dashboards, and more. Paired with a variety of ways to publish and share your content, it is an excellent platform for data storytelling.\nDeciding to learn R vs. Python is a well-covered topic and often one prone to heated debate. In Quarto, there’s no “Bad Blood” between the two popular programming languages, where you can choose to run your project in R, Python, or both. It’s also compatible with the Julia and Observable JS languages as well as many of the most popular integrated development environments (IDEs) used in the field of data science, like VS Code, Jupyter, and RStudio. This flexibility means data scientists can collaborate on projects using the tools of their choice."
  },
  {
    "objectID": "gallery/taylor-swift/analysis.html#how-quarto-generated-the-eras-tour-data",
    "href": "gallery/taylor-swift/analysis.html#how-quarto-generated-the-eras-tour-data",
    "title": "Taylor Swift and Data Science: An Unlikely Duo",
    "section": "How Quarto Generated the Eras Tour Data",
    "text": "How Quarto Generated the Eras Tour Data\nYou may have noticed the See the code in R link in the left sidebar of Posit’s article that takes you to a virtually identical page. The key difference is this page allows you to see the code behind the data collection and visualizations. We won’t go line-by-line, but let’s look at the high-level steps they took to craft the “GDP of Taylor” data visualization toward the top of the article.\nExpand the “See R code” section just above “The GDP of Taylor” visualization to see the first code chunk where Posit starts by web scraping the Wikipedia page for nominal GDP by country. Web scraping is a technique in which you write code to visit a website and return information or data. Be sure to read the terms and conditions of a website found in the robots.txt file that tells you what information you may scrape.\nSince Taylor was estimated to stimulate the economy by over $6 billion, the collected data is filtered to countries with GDPs between $4 and $10 billion for comparisons of similar magnitude. Next, Posit plots the map and GDP of each of those eight countries using the R library, ggplot2. Lastly, they stitch everything together with Taylor’s image and economic impact in the center using the cowplot library. Starting with several discrete plots and organizing them together, they are able to create an infographic that puts the Eras Tour in shocking perspective.\nThis is a great example of data science in action. As data scientists we’re often asked questions or have hypotheses but are not handed a tidy dataset. Instead, we must connect to an API or find data online, automate the process of collecting it, and manipulate it into a format that will be conducive to our analysis. Data collection and cleaning are often the iceberg below the surface while visualizations and predictive models are the parts everyone can see. Without good data, it’s incredibly difficult to produce insightful analyses."
  },
  {
    "objectID": "gallery/taylor-swift/analysis.html#flatirons-highest-grossing-concert-tours-data-visualization",
    "href": "gallery/taylor-swift/analysis.html#flatirons-highest-grossing-concert-tours-data-visualization",
    "title": "Taylor Swift and Data Science: An Unlikely Duo",
    "section": "Flatiron’s Highest-Grossing Concert Tours Data Visualization",
    "text": "Flatiron’s Highest-Grossing Concert Tours Data Visualization\nLike Posit, we collected the data from the List of highest-grossing concert tours page on Wikipedia. Instead of a static chart, we created a bar chart race—a fun way to visualize data changing over time using animation. Below we have the highest single-year tours by gross revenue from 1993 to 2023.\n\n\n\nData source: Wikipedia\n\n\nThe Rolling Stones and U2 tours held most of the top five spots for a majority of the past 30 years. That is, until the 2023 Eras Tour nearly doubled the $617 million grossed by the A Bigger Bang Tour—the 17-year record-holder set by the Stones in 2006. Interestingly, Taylor Swift is the first female solo artist to crack the list since Madonna’s The MDNA Tour in 2012. With the Eras Tour projected to bring in another $1 billion in 2024, it will likely be Taylor Swift in the top two spots come end of year.\nThis analysis was originally created in our own internal Quarto project at Flatiron School and copied over here onto our blog. Give Quarto a try and you might just tell Jupyter notebooks and RMarkdown, “We Are Never Ever Getting Back Together.”"
  },
  {
    "objectID": "gallery/french-open/analysis.html",
    "href": "gallery/french-open/analysis.html",
    "title": "Quantifying Rafael Nadal’s Dominance with French Open Data",
    "section": "",
    "text": "This article was originally posted to Flatiron School’s Blog, but has since been moved"
  },
  {
    "objectID": "gallery/french-open/analysis.html#french-open-titles",
    "href": "gallery/french-open/analysis.html#french-open-titles",
    "title": "Quantifying Rafael Nadal’s Dominance with French Open Data",
    "section": "French Open Titles",
    "text": "French Open Titles\nIn 18 career appearances, Rafael Nadal has won the French Open 14 times. The next closest male is Björn Borg with six titles. On the female side, Chris Evert holds the record with seven.\nTaking our scope beyond just the French Open, no player, male or female, has 14 singles titles at any one Grand Slam tournament. Aside from Nadal, the only active player in the table below is Novak Djokovic, who holds the closest active record with 10 Australian Open titles. Already playing the tournament 19 times, career longevity becomes a challenge if he were to unseat Nadal as the winningest player at a single major tournament.\nAmong these tennis greats, Nadal’s stretch of success at the French Open is truly eye-catching.\n\n\n\nThe table above comes from Flatiron’s analysis of individual titles at a single Grand Slam tournament in the Open Era (1968 to present)."
  },
  {
    "objectID": "gallery/french-open/analysis.html#what-makes-him-so-dominant",
    "href": "gallery/french-open/analysis.html#what-makes-him-so-dominant",
    "title": "Quantifying Rafael Nadal’s Dominance with French Open Data",
    "section": "What makes him so dominant?",
    "text": "What makes him so dominant?\nA unique aspect of the French Open is its surface. Rather than the traditional blue or green hard court (typically concrete) you’re likely to find at a nearby park or sports complex, Roland Garros features an orange-red surface made of densely packed clay. This surface results in a distinct gameplay that rewards defensive play and makes the ball behave differently off the bounce compared to other surfaces. Another challenge posed by clay is the reduced friction between the shoe and the surface, requiring players to slide into position to strike the ball as they move around the court.\n\n\n\nAn image of Rafael Nadal on the clay court at the French Open. Source: rolandgarros.com\n\n\nMany experts attribute Nadal’s success at the French Open to his athleticism and emphasis on power and spin off the racket. These characteristics, accentuated by the clay court, allow him to hit returns other players cannot and to remain on the offensive even while his opponent is serving.\nThe “Big Three”—comprised of Novak Djokovic, Roger Federer, and Rafael Nadal—is the nickname for the trio considered the greatest male tennis players of all time. Even among this group Nadal’s return statistics stand out. In French Open matches, he wins an average of 49% of return points compared to Djokovic’s 44% and Federer’s 41%. Winning nearly 50% of return points is unheard of, especially when top players are expected to win 70% or more of the points in which they are serving.\nAdditionally, Nadal’s median Ace Rate Against—a measure of how often a player is unable to touch their opponent’s serve—is just 2.7% at the French Open, the lowest among all three athletes at any Grand Slam. To take the opponent’s serving advantage away this dramatically, it’s clear evidence of Nadal’s impressive upper hand on the clay court.\nThe figure below compares match statistics for Djokovic, Federer, and Nadal at Grand Slam tournaments across their respective careers.\n\n\n\nThe chart above comes from Flatiron’s analysis of career matches at Grand Slam tournaments for Rafael Nadal, Roger Federer, and Novak Djokovic, collectively known as the ‘Big Three’."
  },
  {
    "objectID": "gallery/french-open/analysis.html#the-greatest-records-of-all-time",
    "href": "gallery/french-open/analysis.html#the-greatest-records-of-all-time",
    "title": "Quantifying Rafael Nadal’s Dominance with French Open Data",
    "section": "The Greatest Records of All Time",
    "text": "The Greatest Records of All Time\nThe data so far shows that no tennis player has dominated one of the majors the way Nadal has at the French Open, but how does his record compare to non-tennis records? What methodology could we use to compare apples and oranges, or perhaps, tennis balls, basketballs, and hockey pucks?\nThere are a number of ways to approach any given problem in the field of Data Science. In fact, for a field known for its quantitative rigor, there are many aspects that allow for creativity. Designing data visualizations, weaving insights into a cohesive story, or, in our case, developing a methodology for comparing athletic achievements are all ways in which creative thinking is an asset.\nFor our problem, we could visualize the difference relative to the next best record-holder or compare the length of time previous records were held. These are interesting ideas, but really only compare two data points head-to-head. With sample size in mind, let’s try to contextualize how far out of the ordinary Nadal’s 14 championship wins are and do the same for a couple other sports achievements.\nThe number of wins at a tennis tournament are of a different magnitude than, say, the number of career points in basketball. To make things fair, we need to standardize. The “standard score”, sometimes called a “Z-score”, is a way for us to compare data measured on different scales. It can be calculated by taking each data point, \\(x\\), subtracting the average, \\(\\bar{x}\\), of data points from the same sample, and dividing by the standard deviation, \\(S\\), a measure of how much variability there is in our data. Written in equation form, we have: \\(z = \\frac{x - \\bar x}{S}\\).\nAs an example, on the top 50 list of most grand slam titles at a single tournament, the average is 2.2 with a standard deviation of 2.1. Therefore, the z-score for Rafael Nadal’s French Open record is:\n\\[\nz = \\frac{14 \\space titles - 2.2 \\space average}{standard \\space deviation \\space of \\space 2.1} = 5.6\n\\]\nZ-scores are unitless, meaning we can calculate and compare these for records from different categories, even different sports. It is important, however, to know that z-scores can be susceptible to skewed data and to confidently say if one is more extreme than another, we may need to account for distributional characteristics before standardizing them. We will save those techniques for another time, but for now, we can say with certainty that a positive z-score means a data point is atypically high relative to the population from which it comes. Conversely, a negative one means it is unusually low compared to its population. In summary, the further a z-score is from zero, the further it is from average.\nIn the chart below, we compare Nadal’s record with three notable records. Namely, the National Hockey League career points record set by Wayne Gretzky, the recently-set women’s college basketball scoring record by Caitlin Clark, and Katie Ledecky’s ever-growing count of gold medals at the Olympics and World Championships.\nIt’s clear that all four athletes’ achievements stand far beyond the competition. Nadal’s 14 titles put him farthest from average of these four records, but the right-skewed distributions make this an imperfect comparison. Further analysis could include exploring measures that are more robust to skew and outliers.\n\n\n\nThe chart above comes from Flatiron’s analysis comparing Nadal’s 14 French Open titles to other athletic records."
  },
  {
    "objectID": "gallery/french-open/analysis.html#whats-next",
    "href": "gallery/french-open/analysis.html#whats-next",
    "title": "Quantifying Rafael Nadal’s Dominance with French Open Data",
    "section": "What’s next?",
    "text": "What’s next?\nIs Nadal’s record of 14 titles at a single Grand Slam tournament the most impressive athletic feat of all time? One could certainly argue it is. Can it be broken? Only time will tell. As we’ve seen, even among tennis legends like Serena Williams, Roger Federer, and Novak Djokovic, Rafael Nadal stands alone more than 5.6 standard deviations above the average. Perhaps if a young, up-and-coming player can perfect power, spin, and mobility on clay, they could make a run for his French Open record. That is, if they can also remain at the top of their game over a multi-decade career, the way Rafael “The King of Clay” Nadal has. ¡Vamos Rafa!"
  },
  {
    "objectID": "gallery/french-open/analysis.html#learn-data-science-at-flatiron",
    "href": "gallery/french-open/analysis.html#learn-data-science-at-flatiron",
    "title": "Quantifying Rafael Nadal’s Dominance with French Open Data",
    "section": "Learn Data Science at Flatiron",
    "text": "Learn Data Science at Flatiron\nUnlocking the power of data goes beyond basic visualizations. Our Data Science Bootcamp dives deep into data visualization techniques, alongside machine learning, data analysis, and much more. Equip yourself with the skills to transform data into insightful stories that drive results. Visit our website to learn more about our courses and how you can become a data scientist."
  },
  {
    "objectID": "posts/2024-07-16-data-quest/index.html",
    "href": "posts/2024-07-16-data-quest/index.html",
    "title": "Inaugural Mastery Quest - Summer Olympics",
    "section": "",
    "text": "This week’s Data Science Mastery Quest, or Data Quest, was to find a dataset on the Summer Olympics and create a visualization, model, app, or other data-related output using the tool of your choice.\nBelow is my contribution followed by the how-to, for those interested."
  },
  {
    "objectID": "posts/2024-07-16-data-quest/index.html#how-i-made-it",
    "href": "posts/2024-07-16-data-quest/index.html#how-i-made-it",
    "title": "Inaugural Mastery Quest - Summer Olympics",
    "section": "How I Made It",
    "text": "How I Made It\nI started with the research question, what countries have been the most dominant all-time?\nTo start, I found this data from an old Kaggle contest. In order to manipulate and visualize it, I’m loading the tidyverse of packages and the emoji package for fun graphics.\n\nlibrary(tidyverse)\nlibrary(emoji)\nlibrary(ggrepel)\nlibrary(ggtext)\nlibrary(ggimage)\n\nolympics &lt;- readr::read_csv('athlete_events.csv') %&gt;% rename_with(tolower)\nnoc &lt;- readr::read_csv(\"noc_regions.csv\") %&gt;% rename_with(tolower)\n\nTaking a peek at the data, I can see it’s one row per athlete per year per event. I also see it has the Winter Olympics included. I’ll want to filter these out considering the Summer Olympics theme. I’ll also need to count only one medal per event so team sports like basketball and relays don’t skew the counts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nname\nsex\nage\nheight\nweight\nteam\nnoc\ngames\nyear\nseason\ncity\nsport\nevent\nmedal\n\n\n\n\n1\nA Dijiang\nM\n24\n180\n80\nChina\nCHN\n1992 Summer\n1992\nSummer\nBarcelona\nBasketball\nBasketball Men’s Basketball\nNA\n\n\n2\nA Lamusi\nM\n23\n170\n60\nChina\nCHN\n2012 Summer\n2012\nSummer\nLondon\nJudo\nJudo Men’s Extra-Lightweight\nNA\n\n\n3\nGunnar Nielsen Aaby\nM\n24\nNA\nNA\nDenmark\nDEN\n1920 Summer\n1920\nSummer\nAntwerpen\nFootball\nFootball Men’s Football\nNA\n\n\n4\nEdgar Lindenau Aabye\nM\n34\nNA\nNA\nDenmark/Sweden\nDEN\n1900 Summer\n1900\nSummer\nParis\nTug-Of-War\nTug-Of-War Men’s Tug-Of-War\nGold\n\n\n5\nChristine Jacoba Aaftink\nF\n21\n185\n82\nNetherlands\nNED\n1988 Winter\n1988\nWinter\nCalgary\nSpeed Skating\nSpeed Skating Women’s 500 metres\nNA\n\n\n5\nChristine Jacoba Aaftink\nF\n21\n185\n82\nNetherlands\nNED\n1988 Winter\n1988\nWinter\nCalgary\nSpeed Skating\nSpeed Skating Women’s 1,000 metres\nNA\n\n\n\n\n\nI’d like to visualize cumulative medal counts by country to see if there are stretches of dominance. To do this, I’ll need to aggregate the athlete-event data by country and year. I also need a reference table for the country flag emojis.\n\ncumulative_medals &lt;- \n  olympics %&gt;% \n  left_join(\n    noc,\n    by = \"noc\"\n  ) %&gt;% \n  filter(season == \"Summer\", !is.na(medal)) %&gt;% \n  # Rename UK and USA for emoji match later\n  mutate(\n    team = case_when(\n      noc == \"GBR\" ~ \"United Kingdom\",\n      noc == \"USA\" ~ \"United States\",\n      TRUE ~ region\n    )\n  ) %&gt;% \n  # Aggregate medals by country and year\n  group_by(team, year, medal) %&gt;% \n  summarise(n = n_distinct(event), .groups = \"drop\") %&gt;% \n  pivot_wider(names_from = medal, values_from = n, values_fill = 0) %&gt;% \n  mutate(Total = Gold + Silver + Bronze) %&gt;% \n  group_by(team) %&gt;% \n  arrange(team, year) %&gt;% \n  # Create a cumulative sum\n  mutate(across(Bronze:Total, ~ cumsum(replace_na(.x, 0))))\n\ncountry_flags &lt;- \n  emojis %&gt;% \n  filter(subgroup == \"country-flag\") %&gt;% \n  mutate(country = str_extract(name, \"(?&lt;=flag: ).*\"))\n\nWith these aggregate tables, I’m ready to plot. See inline comments for detail.\n\nby_country &lt;-\n  cumulative_medals %&gt;% \n  # Only show countries with at least 500 total medals\n  filter(max(Total) &gt;= 500) %&gt;% \n  left_join(country_flags, by = c(\"team\" = \"country\")) %&gt;% \n  group_by(team) %&gt;% \n  # Only label the last year with the flag emoji to avoid noise\n  mutate(label = if_else(year == max(year), emoji, NA_character_)) %&gt;% \n  ggplot(aes(x = year, y = Total)) + \n  # Highlight Russia/USSR in red during Cold War years\n  geom_line(\n    aes(\n      group = team,\n      color = if_else(\n        team == \"Russia\" & year &gt;= 1947 & year &lt;= 1991,\n        \"#FF3C28\",\n        \"grey60\"\n      )\n    )\n  ) +\n  scale_color_identity() +\n  geom_text_repel(\n    aes(label = label), \n    family = \"DIN Alternate\",\n    hjust = 0,\n    vjust = 0.5,\n    nudge_y = 15,\n    segment.size = 0.2,\n    xlim = c(2016, NA)\n  ) +\n  # Set ticks to years divisible by four to align with years in which Summer Games were held\n  scale_x_continuous(limits = c(1896, 2024), breaks = seq(1896, 2024, by = 16)) +\n  scale_y_continuous(\n    limits = c(0, 3000),\n    labels = scales::comma,\n    position = \"right\") +\n  labs(\n    title = \"&lt;b&gt;The Cold War: A Space Race or Foot Race?&lt;/b&gt;&lt;br&gt;The &lt;span style='color: #ff3c28'&gt;USSR&lt;/span&gt;&lt;sup&gt;1&lt;/sup&gt; wins over 1,000 medals -- the most of any country in this period\",\n    subtitle = \"Cumulative Summer Olympics medal count by country for countries with at least 500 medals\",\n    x = NULL,\n    y = NULL,\n    caption = \"Data source: Kaggle&lt;br&gt;&lt;sup&gt;1&lt;/sup&gt; USSR/Russia are combined as are Germany/East Germany for this analysis\") +\n  # Add Cold War annotation\n  geom_errorbarh(\n    inherit.aes = FALSE,\n    data = tibble(xmin = 1947, xmax = 1991, y = 2600),\n    aes(xmin = xmin, xmax = xmax, y = y),\n    height = 100\n  ) +\n  annotate(\n    \"text\", x = 1969, y = 2700, \n    label = \"The Cold War (1947-1991)\", \n    family = \"DIN Alternate\",\n    hjust = 0.5, vjust = 0\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(margin = margin(l = 100, b = 5), vjust = 0.5),\n    plot.subtitle = element_markdown(margin = margin(l = 100, b = 10)),\n    plot.caption = element_markdown(),\n    text = element_text(family = \"DIN Alternate\"),\n    axis.line = element_line(color = \"grey20\"),\n    strip.text = element_text(size = 48),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n    )  +\n  coord_cartesian(clip = 'off') +\n  # Add Olympic Rings logo\n  annotation_custom(\n    magick::image_read(\"olympic_rings.svg\") %&gt;% grid::rasterGrob(interpolate = TRUE), \n    x = 1890, xmax = 1914, y = 3300, ymax = 4100)\n\nggsave(by_country, file = \"by-country.svg\", width = 8, height = 4)\n\n\n\n\nIt’s widely known that the United States was in a Space Race with the Soviet Union throughout the Cold War, but it turns out we were also in a foot race. The USSR accumulated 1,005 golds, silvers, and bronzes starting in the 1952 Games in Helsinki through the 1988 Games in Seoul. The next closest countries were the USA and Germany with 957 and 755 medals tallied, respectively."
  },
  {
    "objectID": "posts/2019-05-08-nhlWebscraping/index.html",
    "href": "posts/2019-05-08-nhlWebscraping/index.html",
    "title": "Webscraping NHL Data with RSelenium, Part 1",
    "section": "",
    "text": "I love hockey. I also love data science. What’s better than merging the two and learn how to use RSelenium in the process?\nA while ago, I wrote some web-scraping code to find the NHL standings as of Novermber 1st (roughly one month into the season). I was doing this in order to determine if a strong start to the season was related to end-of-season success. Fortunately, the website dropyourgloves.com had convenient and uniform URLs to simply combine season and date strings to create a valid URL to scrape the corresponding HTML table. Unfortunately, the website no longer exists!\nAfter some Google-Fu, I was unable to find a replacement website, however, I found shrpsports.com, which has an easy-to-use dropdown UI, submit bottons, and resulting HTML table. This isn’t as easy as find and replace in the code, so I needed to load RSelenium for automating the browser to populate dropdowns and click submit.\nI found that the preferred way to run RSelenium is via a headless browser running in Docker. This was the first case I found myself needing either technology. I still have work to do to write more robust code, but I got this working, so I wanted to post about it.\nSet up Docker\n\nFirst, I downloaded Docker here\nTo start a headless browser, I ran docker run -d -p 4445:4444 selenium/standalone-chrome (the first time pulls the Docker image from DockerHub)\n\nConnecting to the browser\nThe R code below begins the webscraping journey.\n\n# Import packages\nlibrary(RSelenium)\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Start a docker container with Google Chrome on port 4444 on\n#   the server side inside the container and 4445 on my local machine\nsystem(\"docker run -d -p 4445:4444 selenium/standalone-chrome\")\n\n# Access the remote browser\nremDr &lt;- RSelenium::remoteDriver(remoteServerAddr = \"localhost\",\n                                 port = 4445L,\n                                 browserName = \"chrome\")\n\n# Initialize a browsing session\nremDr$open(silent = TRUE)\n\n# Navigate to the website to scrape\nremDr$navigate(\"http://www.shrpsports.com/nhl/stand.htm\") \n\n# Save a screenshot and display below\nremDr$screenshot(file = \"screenshot.png\")\n\n\nHere’s what the website looks like:\n\n\n\nScrape NHL Standings\nNow for the fun part. I wrote this function to fetch standings from the remote browser for any season and date or for season-end standings. There are a few pitfalls of this function regarding the NHL changing playoff format, conference assignments, etc. that will take quite a bit of elbow grease for a truly robust function, so I’ll save that for another time. I also noticed a few data consistency issues, but hey, it’s a free site from which I’m pulling data.\nFor the timebeing, this function simply returns the Eastern, Western, and, if pulling season-end standings, the Stanley Cup match-up. In a future post, I hope to try the analysis of “Does a strong start to the season predict not only a playoff berth, but also playoff success?” Having this function will allow for that to happen with far less repetitive programming.\n\ngetStandings &lt;- function(season, month, date) {\n  # Gets the NHL standings for any season on any date or final conference standings\n  #\n  # Args:\n  #   season: Four-character string representing the NHL season (year in which Stanley\n  #     Cup is played for the season, e.g. \"2018\" is for the 2017-18 season)\n  #   month (optional): Three-character month abbreviation\n  #   date (optional): Character representing day of the month (e.g. \"1\", \"12\", \"27\")\n  #\n  # Returns:\n  #   The NHL standings in a dataframe\n  \n  # Enter the URL for the browser\n  remDr$navigate(\"http://www.shrpsports.com/nhl/stand.htm\") \n  \n  # Save the homepage HTML to reuse in several \n  homepage &lt;- read_html(remDr$getPageSource()[[1]])\n  \n  # Get seasons from dropdown\n  valid_seasons &lt;- homepage %&gt;% \n    html_nodes(\"select[name='season']\") %&gt;% \n    html_children() %&gt;% \n    html_attr(\"value\")\n  \n  # Get months from dropdown\n  valid_mos &lt;- homepage %&gt;% \n    html_nodes(\"select[name='month']\") %&gt;% \n    html_children() %&gt;% \n    html_attr(\"value\")\n  \n  # Get days of month from dropdown\n  valid_dates &lt;- homepage %&gt;% \n    html_nodes(\"select[name='date']\") %&gt;% \n    html_children() %&gt;% \n    html_attr(\"value\")\n  \n  # Verify season input\n  if (!(season %in% valid_seasons)) stop(\"Invalid season\")\n  \n  # Determine if user wants final standings or standings as of a date\n  if (missing(month) | missing(date)) {\n    div_conf &lt;- \"latefincnf\"\n    month &lt;- \"\"\n    date &lt;- \"\"\n    message(\"Getting season-end standings...\")\n  } else {\n    if (!(month %in% valid_mos)) stop(\"Invalid month\")\n    if (!(date %in% valid_dates)) stop(\"Invalid date\")\n    div_conf &lt;- \"cnf\"\n    message(glue(\"Getting standings as of {month}-{date}...\"))\n  }\n  \n  # Select season input in dropdown\n  season &lt;- remDr$findElement(using = 'css selector', \n                              glue(\"select[name='season'] option[value='{season}']\"))\n  season$clickElement()\n  \n  # Select division/conference in dropdown\n  divcnf &lt;- remDr$findElement(using = 'css selector', \n                              glue(\"select[name='divcnf'] option[value='{div_conf}']\"))\n  divcnf$clickElement()\n  \n  # Select month in dropdown\n  month &lt;- remDr$findElement(using = 'css selector', \n                             glue(\"select[name='month'] option[value='{month}']\"))\n  month$clickElement()\n  \n  # Select day of month in dropdown\n  dom &lt;- remDr$findElement(using = 'css selector', \n                           glue(\"select[name='date'] option[value='{date}']\"))\n  dom$clickElement()\n  \n  # Click submit botton\n  submit &lt;- remDr$findElement(using = 'css selector', \"input[type='submit']\")\n  submit$clickElement()\n  \n  # NOT RUN: This will take a screenshot of the current remote browser\n  #   screen and display it in the RStudio viewer\n  # remDr$screenshot(display = TRUE) \n  \n  # Read the HTML table from resulting webpage\n  raw_table &lt;- read_html(remDr$getPageSource()[[1]]) %&gt;% \n    html_table(fill = TRUE) %&gt;% \n    .[[3]]\n  \n  # Names are stored in the second row, so rename the table accordingly\n  names(raw_table) &lt;- raw_table[2,]\n  \n  # Clean up column name holding the NHL team name and remove excess rows\n  raw_table &lt;- raw_table %&gt;% \n    rename(Team = \"\") %&gt;% \n    filter(Team != \"\") %&gt;% \n    rename_all(~str_replace_all(.,\"\\\\-\", \"\\\\_\"))\n  \n  # Index the rows of the table holding Conference subheadings\n  conf_idx &lt;- raw_table$Team %&gt;% \n    grep(\"conf\", ., ignore.case = TRUE)\n  \n  # Store all Eastern conference results in a dataframe\n  east &lt;- raw_table %&gt;% \n    slice(1:(conf_idx[2] - 1)) %&gt;% \n    filter(!str_detect(Team, regex(\"conf\", ignore_case = TRUE))) %&gt;% \n    mutate(place = row_number(),\n           Team = str_trim(str_replace(Team, \"\\\\*|\\\\d\", \"\")),\n           playoffs = if_else(place &lt;= 8, TRUE, FALSE))\n  \n  # Store all Western conference results in another dataframe\n  west &lt;- raw_table %&gt;% \n    slice(conf_idx[2]:n()) %&gt;% \n    filter(!str_detect(Team, regex(\"conf\", ignore_case = TRUE))) %&gt;% \n    mutate(place = row_number(),\n           Team = str_trim(str_replace(Team, \"\\\\*|\\\\d\", \"\")),\n           playoffs = if_else(place &lt;= 8, TRUE, FALSE))\n  \n  # If user wants final conference standings, also get Stanley Cup match\n  if (div_conf == \"latefincnf\") {\n    \n    sc_match &lt;- read_html(remDr$getPageSource()[[1]]) %&gt;%\n      html_table(fill = TRUE) %&gt;%\n      tail(1) %&gt;%\n      .[[1]] %&gt;% \n      filter(!str_detect(X1, regex(\"cup\", ignore_case = TRUE))) %&gt;%\n      mutate(X1 = str_remove_all(X1, \"[\\\\d\\\\-]\") %&gt;% \n               str_remove(\"\\\\w+$\") %&gt;% \n               str_trim()) %&gt;%\n      separate(X1, c(\"Winner\", \"Loser\"), \"  \")\n    \n  }\n  \n  return(\n    list(eastern = east,\n         western = west,\n         stanley_cup = if (exists(\"sc_match\")) sc_match else NA)\n  )\n  \n}\n\nBelow are sample results of running the function. Looks like it works for season-end standings!\n\n# Let's get the season-end Eastern standings for the 2016 season\ngetStandings(\"2016\")$eastern %&gt;% \n  kable() %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n                full_width = FALSE, font_size = 12)\n\nLet’s see how my Detroit Red Wings were doing on the day I was born…\n\ndrw &lt;- getStandings(\"1993\", \"Feb\", \"14\")$western %&gt;% \n  filter(Team == \"Detroit\")\n\ndrw %&gt;% \n  kable() %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n                full_width = FALSE, font_size = 12)\n\nA record of 31-21-7…not bad. Fun fact, Montreal won the cup that year.\nThat’s it for now, thanks for reading. Hopefully more to come on this."
  },
  {
    "objectID": "posts/2019-05-13-raspberry-pi/index.html",
    "href": "posts/2019-05-13-raspberry-pi/index.html",
    "title": "Raspberry Pi Dog Monitor",
    "section": "",
    "text": "Crate-training a puppy can be tough and a total surprise when you come home to find that they’ve torn up their bed, had an accident, or worse, broken free and vandalized your apartment. My dog, Harper, has done all three, but has since learned to love her cozy den. As I teased in my “Hello World” post, I built a dog monitor using a Raspberry Pi to see what she was up to while I was gone. She didn’t break out this time, but she was a bit mischievous.\nThe hardware requirements for this project are a Raspberry Pi and camera. If you need help setting up the camera, PyImageSearch is a great resource and can help you set it up. If you’ve never read it before, I highly recommend the blog. It has tons of posts and resources for learning OpenCV and applying deep learning computer vision techniques. It’s where I caught the computer vision bug and found inspiration for several projects.\nI still haven’t figured out how to edit mp4/mov files into timelapse videos, so instead I took still photos at set time intervals and stitched them together into a gif with a brief delay. To do this, I used ImageMagick. On the Raspberry Pi, it was as simple as:\nsudo apt-get update\nsudo apt-get install imagemagick\nOnce I set up my RPi and camera, I switched over to Python. Below are the necessary packages for this project. Something worth calling out is that with the smtplib package, Gmail can get mad about “less secure apps” trying to sign in. Although not a good long-term solution, the workaround for this is to go into your Google security settings and allowing these apps.\n# General use packages\nimport os\nimport time\nimport getpass\n# Packages needed for RPi Camera\nfrom picamera.array import PiRGBArray\nfrom picamera import PiCamera\n# Packages for emailing the video captured\nimport smtplib\nfrom email import encoders\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nfrom email.mime.base import MIMEBase\nNext, I set the email address from which I wanted to send the timelapse gif as well as the list of addresses to which I wanted to send it. I also initialized the SMTP connection and authenticated in this step.\n# Set email address to send from,\n#   email address (list) to send to, and \n#   initialize SMTP connection to Gmail\nfromaddr = \"beebe.mitch@gmail.com\"\nemailTo = [\"beebe.mitch@gmail.com\"]\nserver = smtplib.SMTP('smtp.gmail.com', 587)\nserver.starttls()\nserver.login(fromaddr, getpass.getpass(\"Enter password: \"))\nNow that the email connection was set up, I cleared out any files associated with previous executions. I then initialized the RPi camera and begain taking still photos at 60 second intervals for 60 minutes * 4 hours, saving each photo in the same directory as image0000.jpg, image0001.jpg, image0002.jpg, and so on.\n# Delete old gifs\nif os.path.isfile(\"harperMonitor.gif\"):\n    os.remove(\"harperMonitor.gif\")\n\n# Delete old jpg images saved\nfilelist = [ f for f in os.listdir(\".\") if f.endswith(\".jpg\") ]\nfor f in filelist:\n    os.remove(f)\n\n# Initialize camera\ncamera = PiCamera()\ncamera.resolution = (1024, 768)\n\n# Take a still jpg picture every minute for 4 hours\n#   and save image with 4-digit suffix with image index\nfor i in range(60*4):\n    camera.capture('image{0:04d}.jpg'.format(i))\n    time.sleep(60)\nWhen that loop finished, I then executed the code below from Python via the command line. Convert is a command from ImageMagick that allows you to manipulate image formats. I told it to convert every file with the .jpg extension into a gif with a tenth of a second delay between each image and no loop.\n# Convert jpg to gif with 0.1 second delay\nos.system('convert -delay 10 -loop 0 image*.jpg harperMonitor.gif')\nNext, I initialized a MIMEMultipart message with proper to and from emails, subject, and body. Lastly, I opened the gif and attached it using the MIMEImage subclass. Now the email was ready to send and close the connection.\n# Initialize a multipart email (one with body, attachments, etc) \nmsg = MIMEMultipart()\n\n# Add From, To, Subject, and Body of email\nmsg['From'] = fromaddr\nmsg['To'] = \", \".join(toaddr)\nmsg['Subject'] = \"Harper Monitor for \" + time.strftime(\"%m/%d/%y\")\nbody = \"Here is your daily Harper video! Woof.\"\nmsg.attach(MIMEText(body, 'plain'))\n\n# Attach the gif to the email\nfp = open(\"harperMonitor.gif\",\"rb\")\nmsgImg = MIMEImage(fp.read())\nfp.close()\nmsg.attach(msgImg)\n\n# Send the email and close the connection\nserver.sendmail(fromaddr, toaddr, msg.as_string())\nserver.quit()\nThat’s it! It takes ~ 4 hours to run (duh), but you can alter the frequency and number of images taken depending on how long Harper is in her crate. Below is a cropped version of the end result that I received in my Gmail inbox. Someone got ornery while I was away! Thanks for reading."
  },
  {
    "objectID": "posts/2019-05-07-hello-world/index.html",
    "href": "posts/2019-05-07-hello-world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "This is my first post! This website is developed with Quarto and served via Netlify. I tried hosting this on GitHub Pages, but had too much difficulty customizing. Having used Blogdown, I’m making the transition to Quarto.\nSo far this is great. I can embed R code like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nOr include plots:\n\npie(\n  c(280, 60, 20),\n  c('Sky', 'Sunny side of pyramid', 'Shady side of pyramid'),\n  col = c('#0292D8', '#F7EA39', '#C4B632'),\n  init.angle = -50, border = NA\n)\n\n\n\n\n\n\n\n\n\nI don’t anticipate blogging a lot, but I hope to make this a platform to showcase some of my pet projects. Fair warning, a lot of my projects revolve on my dog, Harper, so I literally mean “pet”. Below are a few examples I hope to post about soon:\n\nRaspberry Pi timelapse video of Harper crate-training\nHarper/Not Harper image classification mobile app\nHarper autoencoder to generate new, “fake” Harper images\n\nThanks for reading!"
  },
  {
    "objectID": "posts/2019-05-07-hello-world/index.html#quarto",
    "href": "posts/2019-05-07-hello-world/index.html#quarto",
    "title": "Hello World",
    "section": "",
    "text": "This is my first post! This website is developed with Quarto and served via Netlify. I tried hosting this on GitHub Pages, but had too much difficulty customizing. Having used Blogdown, I’m making the transition to Quarto.\nSo far this is great. I can embed R code like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nOr include plots:\n\npie(\n  c(280, 60, 20),\n  c('Sky', 'Sunny side of pyramid', 'Shady side of pyramid'),\n  col = c('#0292D8', '#F7EA39', '#C4B632'),\n  init.angle = -50, border = NA\n)\n\n\n\n\n\n\n\n\n\nI don’t anticipate blogging a lot, but I hope to make this a platform to showcase some of my pet projects. Fair warning, a lot of my projects revolve on my dog, Harper, so I literally mean “pet”. Below are a few examples I hope to post about soon:\n\nRaspberry Pi timelapse video of Harper crate-training\nHarper/Not Harper image classification mobile app\nHarper autoencoder to generate new, “fake” Harper images\n\nThanks for reading!"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Inaugural Mastery Quest - Summer Olympics\n\n\n\n\n\nLearn more on Bletchley.org!\n\n\n\n\n\nJul 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWebcam Image Classification\n\n\n\n\n\n\nJavascript\n\n\n\n\n\n\n\n\n\nMay 21, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nRaspberry Pi Dog Monitor\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nMay 13, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping NHL Data with RSelenium, Part 1\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nHello World\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 7, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-05-21-webcam/index.html",
    "href": "posts/2019-05-21-webcam/index.html",
    "title": "Webcam Image Classification",
    "section": "",
    "text": "As I mentioned in my “Hello World” post, I’m fascinated with computer vision. I’m in the process of writing up a post about the image classification mobile app that I made to detect whether my iPhone camera is pointing at my dog or not. While trying to figure out how to host a live ML model within a browser, I learned how to call pre-trained Tensorflow models from the HTML &lt;script&gt; tag. This option is pretty straightforward, but required learning a bit of JavaScript. If I tried to do this strictly in R, I anticipated embedding a Shiny application with an upload photo UI, pre-loaded model, image display, etc.\nIn this post, I create a webcam image classifier using a pre-trained TensorFlow.js MobileNet model hosted on jsDelivr. MobileNets are relatively small neural networks that are optimized to work well on less powerful computers (e.g. cell phones, tablets). This increased efficiency comes with a tradeoff in accuracy. Depending on the exact architecture chosen, MobileNets tend to top out around 60-70% accuracy. This particular model can predict 1,000 different classes, mostly commonplace objects and animals. I’ve found that it gets confused when the background is non-monochromatic, so for best results try to position the object in front of something simple. Before diving into the code, I am by no means a front end developer, so forgive the weak UI and delay in classification and display of the results. Below is the final product, followed by the instructions to recreate it. Have fun!\n\nThe final product!\n\n  Capture video\n  Take screenshot\n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\nHow it’s made:\nThe first step is to load the necessary libraries:\n\nTensorFlow.js to run the model\nThe MobileNet model to classify the images\nD3.js for visualization\n\n\n&lt;!-- Load TensorFlow.js. This is required to use MobileNet. --&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.1\"&gt;&lt;/script&gt;\n&lt;!-- Load the MobileNet model. --&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.1\"&gt;&lt;/script&gt;\n&lt;!-- Load D3 for the visualization of predicted image classifications --&gt;\n&lt;script src=\"https://d3js.org/d3.v4.min.js\"&gt;&lt;/script&gt;\n\n\n\n\n\n\n\nI add some (bare minimum) CSS so that the UI is somewhat presentable.\n\n&lt;style&gt;\n.bar {\n    fill: #27A822;\n}\n.axis {\n    font-size: 10px;\n}\n.axis path,\n.axis line {\n    fill: none;\n    display: none;\n}\n.label {\n    font-size: 13px;\n}\n.column {\n  float: left;\n  width: 33.33%;\n}\n.row:after {\n  content: \"\";\n  display: table;\n  clear: both;\n}\n&lt;/style&gt;\n\n\n\n\n\nNext, I add a basic UI with a “Capture video” and “Take screenshot” button that will start the webcam and screenshot a single frame to initiate object classification, respectively.\n&lt;p align=\"center\"&gt;\n  &lt;button id=\"capture-button\" class=\"capture-button\"&gt;Capture video&lt;/button&gt;\n  &lt;button id=\"screenshot-button\" disabled&gt;Take screenshot&lt;/button&gt;\n&lt;/p&gt;\n&lt;br&gt;\n&lt;div class=\"row\"&gt;\n  &lt;div class=\"column\"&gt;\n    &lt;video class=\"videostream\" width=\"240\" height=\"180\" autoplay&gt;&lt;/video&gt;\n  &lt;/div&gt;\n  &lt;div class=\"column\"&gt;\n    &lt;img id=\"screenshot-img\" width=\"240\" height=\"180\"&gt;\n  &lt;/div&gt;\n  &lt;div class=\"column\"&gt;\n    &lt;div id=\"graphic\" align=\"center\" width=\"240\" height=\"180\"&gt;&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\nNow that the UI is all set up, you need to make it react to input. To start, I initialize several variables that will be used later by the webcam. I also set up the size and axes of the graph that will ultimately display the top 3 classes and corresponding probabilities that the MobileNet predicts for the object in the still screenshot.\n\n// Initialize webcam and image settings\nconst constraints = {\n  video: true\n};\nconst captureVideoButton = document.getElementById('capture-button');\nconst screenshotButton = document.getElementById('screenshot-button');\nconst img = document.getElementById('screenshot-img');\nconst video = document.querySelector('video');\nconst canvas = document.createElement('canvas');\n\n// Set the dimensions and margins of the graph\nvar margin = {top: 0, right: 0, bottom: 25, left: 110},\n    width = 240 - margin.left - margin.right,\n    height = 180 - margin.top - margin.bottom;\n\n// Create the graph with the dimensions set above\nvar svg = d3.select(\"#graphic\").append(\"svg\")\n        .attr(\"width\", width + margin.left + margin.right)\n        .attr(\"height\", height + margin.top + margin.bottom)\n      .append(\"g\")\n        .attr(\"transform\", \n              \"translate(\" + margin.left + \",\" + margin.top + \")\");\n\n// Set the range of y and x\nvar y = d3.scaleBand()\n          .range([height, 0])\n          .padding(0.1);\n\nvar x = d3.scaleLinear()\n          .range([0, width]);\n\n// Initialize the axes with the x-axis formatted as a percentage     \nvar xAxis = d3.axisBottom(x)\n              .ticks(4)\n              .tickFormat(d =&gt; Math.round(d*100) + \"%\");\nvar yAxis = d3.axisLeft(y);\n\n// Attach the settings for the axes\nsvg.append(\"g\")\n    .attr(\"class\", \"y axis\")\n    .call(yAxis);\nsvg.append(\"g\")\n    .attr(\"class\", \"x axis\");\n\nBecause I decided to put the videostream, screenshot, and graph all on the same row of the site, the y-axis labels tended to get cut off, especially when the predicted class is something like “cellular telephone, cellular phone, cellphone, cell, mobile phone”. This next function was taken almost directly from here with minor tweaks to work on a horizontal bar chart rather than a vertical one. It takes text and a fixed width and splits it over several lines.\n\nfunction wrap(text, width) {\n  text.each(function() {\n    var text = d3.select(this),\n        words = text.text().split(/\\s+/).reverse(),\n        word,\n        line = [],\n        lineNumber = 0,\n        lineHeight = 0.5, // ems\n        y = text.attr(\"y\"),\n        dy = parseFloat(text.attr(\"dy\")),\n        tspan = text.text(null).append(\"tspan\").attr(\"x\", -10).attr(\"y\", y).attr(\"dy\", dy + \"em\")\n    while (word = words.pop()) {\n      line.push(word)\n      tspan.text(line.join(\" \"))\n      if (tspan.node().getComputedTextLength() &gt; width) {\n        line.pop()\n        tspan.text(line.join(\" \"))\n        line = [word]\n        tspan = text.append(\"tspan\")\n                        .attr(\"x\", -10)\n                        .attr(\"y\", y)\n                        .attr(\"dy\", `${++lineNumber * lineHeight + dy}em`)\n                        .text(word)\n      }\n    }\n  })\n}\n\nThe last function related to the D3.js graph is below. It updates the existing graph with the data it receives anytime it is called. It’s all pretty self-explanatory, so see the comments within the function.\n\nfunction update(data) {\n    \n    // Sort bars based on value\n    data = data.sort(function (a, b) { \n        return d3.ascending(a.probability, b.probability);\n    });\n    \n    // Format the data\n    data.forEach(function(d) {\n      d.probability = +d.probability;\n    });\n  \n    // Scale the range of the data in the domains\n    x.domain([0, d3.max(data, function(d){ return d.probability; })])\n    y.domain(data.map(function(d) { return d.className; }));\n  \n    // Remove any existing bars on the graph\n    var bars = svg.selectAll(\".bar\")\n        .remove()\n        .exit()\n        .data(data)\n    \n    // Add new bars using the new data\n    bars.enter()\n        .append(\"rect\")\n        .attr(\"class\", \"bar\")\n        .attr(\"width\", function(d) {return x(d.probability); } )\n        .attr(\"y\", function(d) { return y(d.className); })\n        .attr(\"height\", y.bandwidth());\n  \n    // Add the x Axis\n    svg.select(\".x\")\n        .attr(\"transform\", \"translate(0,\" + height + \")\")\n        .call(d3.axisBottom(x)\n                .ticks(4)\n                .tickFormat(d =&gt; Math.round(d*100) + \"%\"));\n  \n    // Add the y Axis\n    svg.select(\".y\")\n        .call(d3.axisLeft(y))\n      .selectAll(\".tick text\")\n        .call(wrap, 100);\n}\n\n\nSo far, I’ve set up the UI and the code to create the graph, but have not set up either the webcam or screenshot mechanism. In the next code chunk, I capture the video stream on button click using the getUserMedia() API. HTML5rocks.com has a very helpful article on video and audio capturing, if you’re interested in learning more. Next, I update the canvas element acting as the placeholder for the screenshot. That screenshot image then gets fed into the MobileNet model and the resulting predictions are passed in as the data to the update() function I wrote above.\n\n// When the video button is clicked, open the video webcam and begin streaming\ncaptureVideoButton.onclick = function() {\n  navigator.mediaDevices.getUserMedia(constraints).\n    then(handleSuccess).catch(handleError);\n};\n\nfunction handleSuccess(stream) {\n  screenshotButton.disabled = false;\n  video.srcObject = stream;\n};\n\n// When the screenshot button or the video itself is clicked,\n// grab a still image of the stream and replace the blank canvas\nscreenshotButton.onclick = video.onclick = function() {\n  \n  canvas.width = video.videoWidth;\n  canvas.height = video.videoHeight;\n  canvas.getContext('2d').drawImage(video, 0, 0);\n  // Other browsers will fall back to image/png\n  img.src = canvas.toDataURL('image/webp');\n  \n  const img_tmp = document.getElementById('screenshot-img');\n  \n  // Load the model\n  mobilenet.load().then(model =&gt; {\n    \n    // Classify the image\n    model.classify(img_tmp).then(predictions =&gt; {\n      \n      // Update the bar chart\n      update(predictions);\n            \n    });\n  });\n};"
  },
  {
    "objectID": "gallery/oscars/analysis.html",
    "href": "gallery/oscars/analysis.html",
    "title": "The Data on Barbie, Greta Gerwig, and Best Director Snubs at the Oscars",
    "section": "",
    "text": "This article was originally posted to Flatiron School’s Blog, but has since been moved\nWhen the 2024 Academy Award nominees were announced in late January, one of the most hotly discussed topics was that Greta Gerwig, director of Barbie, was not nominated for Best Director, despite the film being nominated for Best Picture. I assumed a Best Director nomination went hand-in-hand with a Best Picture nomination, so how common is it for a film to be nominated for Best Picture, but not Best Director? It turns out, fairly often, at least since 2009.\nFrom 1970 to 2008, the Best Picture and Best Director categories had five nominees each. It was common to see four of the five Best Picture nominees also receiving a nomination for Best Director. And in 32 of these 39 years, the film that won Best Picture also won Best Director.\nIn 2009, the Best Picture nomination limit increased to 10 films. Best Director remained capped at five, so naturally, this resulted in more Best Director snubs than before. In terms of winners, the larger pool of Best Picture nominees seems to be aiding in separating the two awards. Best Picture and Best Director Oscars have gone to two different films in six of the last 14 years (this happened only seven times in the 39 years before 2009)."
  },
  {
    "objectID": "gallery/oscars/analysis.html#barbenheimer",
    "href": "gallery/oscars/analysis.html#barbenheimer",
    "title": "The Data on Barbie, Greta Gerwig, and Best Director Snubs at the Oscars",
    "section": "Barbenheimer",
    "text": "Barbenheimer\nAlthough it’s no longer uncommon for a film to receive a Best Picture nomination without one for Best DIrector, Barbie wasn’t just any film. Barbie was one half of the cultural phenomenon known as Barbenheimer. A mashup of two highly anticipated and starkly different films—Barbie, and director Christopher Nolan’s historical biopic Oppenheimer—were both released on July 21, 2023. The goal of seeing both films back-to-back became one of the defining characteristics of the Barbenheimer phenomenon. While both films were hugely successful at the domestic and international box office, Barbie out-grossed Oppenheimer by an estimated half-billion dollars worldwide.\nThe two films dominated the zeitgeist for much of 2023 and both received enormous critical acclaim. Oppenheimer has dominated this awards season, however, with 13 Oscar nominations garnered and multiple important wins at other film awards ceremonies leading up to the Academy Awards on March 10.\n\nWe’ll return to how we think about “importance” in the context of nominations, but for now, let’s compare the two films along the lines of major award ceremonies, ratings, and box office revenue."
  },
  {
    "objectID": "gallery/oscars/analysis.html#barbie-vs-oppenheimer",
    "href": "gallery/oscars/analysis.html#barbie-vs-oppenheimer",
    "title": "The Data on Barbie, Greta Gerwig, and Best Director Snubs at the Oscars",
    "section": "Barbie vs Oppenheimer",
    "text": "Barbie vs Oppenheimer\n\nMinus its take at the People’s Choice Awards, Oppenheimer has taken home more awards overall, despite having a similar number of nominations at most award shows. Barbie appeared to be on a roll this award season, with nominations for picture, director, screenplay, actress, and supporting actor at the Golden Globes and Critics Choice Awards in early January. However, Greta Gerwig was left out of the director category when the Oscar nominees were announced on January 23. This leads to the question, what films are most similar to Barbie, not just by nomination count, but across major categories? And were those films nominated for Best Director?"
  },
  {
    "objectID": "gallery/oscars/analysis.html#movies-like-barbie",
    "href": "gallery/oscars/analysis.html#movies-like-barbie",
    "title": "The Data on Barbie, Greta Gerwig, and Best Director Snubs at the Oscars",
    "section": "Movies Like Barbie",
    "text": "Movies Like Barbie\nWe began our Best Director snubs analysis at Flatiron by collecting all past nominees across the entire history of the awards ceremonies noted in the image above—swapping out the People’s Choice Awards for the Writers Guild Awards—for a comprehensive dataset of non-fan nominations. We also merged categories like Best Adapted Screenplay and Best Original Screenplay into one screenplay category for ease of comparison. Similarly, we lumped all acting categories–male, female, lead, and supporting–into one, and all Best Picture categories into one if split into drama and comedy/musical categories (like the Golden Globes does).\nWith a dataset of over 3,000 nominees going back to the 1920s, we found films most similar to Barbie across our grouped screenplay, grouped actor(s), director, and picture categories using Euclidean distance, a method for finding the distance between two data points. The five films below are the most similar to Barbie according to the awards and groupings we’ve selected. Interestingly, these five films, including Gerwig’s 2017 debut film, Lady Bird, all received a Best Director nomination at the Oscars (while Gerwig’s directing work on Barbie did not)."
  },
  {
    "objectID": "gallery/oscars/analysis.html#predicting-best-director-snubs-at-the-oscars",
    "href": "gallery/oscars/analysis.html#predicting-best-director-snubs-at-the-oscars",
    "title": "The Data on Barbie, Greta Gerwig, and Best Director Snubs at the Oscars",
    "section": "Predicting Best Director Snubs at the Oscars",
    "text": "Predicting Best Director Snubs at the Oscars\nA sample size of five is certainly not enough evidence to make a definitive claim of a snub, so we developed a predictive model that classifies a film as a Best Director nominee based on the other nominations it received, either at the Oscars or previous award shows. Our final model achieved 91% accuracy. For the astute reader, it also reached 93% precision and 96% recall. \nBased on films from 1927 to 2022, the best predictor of a Best Director nomination at the Oscars is a Best Picture nomination at the Oscars. This isn’t surprising, considering the overlap in nominees that we observed in the first image at the top of the article.\n\nOther notable predictors are Best Screenplay at the Oscars or Critics Choice Awards, and Best Director at the Golden Globes or Director’s Guild Awards (DGA). These predictors align with intuition, given the importance of a good script and how common it is to have a filmmaker with the title of writer/director. In the case of the DGA, it’s hard to think of a more qualified group to identify the best directors of the year than the 19,000-plus directors who make up the guild’s membership \nFinally, using our trained model, we applied it to our list of 2023 films that received at least one nomination in a screenplay, acting, directing, or picture category. Given the long list of accolades received by Barbie at the Golden Globes, Critics Choice Awards, British Academy Film Awards (BAFTA), and all the filmmaking guild awards, our model predicted Greta Gerwig to have a 76% chance of snagging a Best Director nomination. Considering she was in third, just behind Christopher Nolan for Oppenheimer and Yorgos Lanthimos for Poor Things, I’d call this a snub. (Gerwig tied for third with Justine Triet for Anatomy of a Fall.)\n\n\n\n\n\n\nBest Director Snubs: Flatiron’s Choice\nRank-ordering the predicted probability of receiving the directorial nomination, the 2017 film Three Billboards Outside Ebbing, Missouri by writer/director Martin McDonagh was our model’s biggest snub. A film that initially received wide acclaim, it later faced criticism over its portrayal of misogyny and racism. Coincidentally, Greta Gerwig was one of the five directors nominees that year alongside Guillermo del Toro, Christopher Nolan, Jordan Peele, and Paul Thomas Anderson—a star-studded list of filmmakers if ever there was one."
  },
  {
    "objectID": "gallery/oscars/analysis.html#final-thoughts",
    "href": "gallery/oscars/analysis.html#final-thoughts",
    "title": "The Data on Barbie, Greta Gerwig, and Best Director Snubs at the Oscars",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nAs with all predictive models, our model is only as good as the data it learns from. A common criticism of the Academy is its lack of nominating women and people of color across categories, particularly for Best Director. Mitigating bias and ensuring fairness in predictive models are important concepts in Big Data Ethics, but we’ll save the ways one could address these issues for another post."
  },
  {
    "objectID": "gallery/index.html",
    "href": "gallery/index.html",
    "title": "Gallery",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nBirdle\n\n\nA bird version of the NY Times’ Wordle. Six guesses to identify the bird of the day.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantifying Rafael Nadal’s Dominance with French Open Data\n\n\nA blog post written for Flatiron School\n\n\n\n\n\n\nMay 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Data on Barbie, Greta Gerwig, and Best Director Snubs at the Oscars\n\n\nA blog post written for Flatiron School\n\n\n\n\n\n\nMar 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTaylor Swift and Data Science: An Unlikely Duo\n\n\nA blog post written for Flatiron School\n\n\n\n\n\n\nJan 24, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "{{< fa chart-line >}} Mitch Beebe",
    "section": "",
    "text": "Welcome to my personal site.\nFind me @mitchbeebe on most platforms or shoot me an email.\n\n\nPowered by Quarto"
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "About",
    "section": "",
    "text": "I am an experienced data scientist and team leader based in New York City. I’m at my best using data to answer business questions and solve problems."
  },
  {
    "objectID": "about/about.html#experience",
    "href": "about/about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nFlatiron School\n\nDirector, Data Science & AnalyticsMar 2022 to Present\nManager, Data Science & AnalyticsOct 2020 to Mar 2022\nSenior Data ScientistAug 2019 to Oct 2020\n\nJPMorgan Chase & Co. | Data ScientistFeb 2018 to Aug 2019\nLuxottica Retail | Senior AnalystDec 2015 to Feb 2018"
  },
  {
    "objectID": "about/about.html#education",
    "href": "about/about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nMiami University | Oxford, OH\n\nMaster of Science in Statistics2014 - 2016\nBachelor of Science in Mathematics & Statistics2011 - 2016"
  },
  {
    "objectID": "about/about.html#skills",
    "href": "about/about.html#skills",
    "title": "About",
    "section": "Skills",
    "text": "Skills\n\n\n    \n        \n            \n        \n            \n        \n            \n        \n            \n        \n            \n        \n        \n         \n        R, SQL\n    \n\n    \n        \n            \n        \n            \n        \n            \n        \n            \n        \n        \n            \n        \n         \n        Python, Airflow, AWS: S3, Lambda, SageMaker, Glue, API Gateway, CloudWatch\n    \n\n    \n        \n            \n        \n            \n        \n            \n        \n        \n            \n        \n            \n        \n         \n        Bash, Git, Hive, Impala\n    \n\n    \n        \n            \n        \n            \n        \n        \n            \n        \n            \n        \n            \n        \n         \n        Django, JavaScript, HTML, CSS\n    \n\n\nNo matching items"
  },
  {
    "objectID": "about/about.html#hobbies",
    "href": "about/about.html#hobbies",
    "title": "About",
    "section": "Hobbies",
    "text": "Hobbies\nJiu jitsu, cooking, traveling"
  },
  {
    "objectID": "about/me.html",
    "href": "about/me.html",
    "title": "About",
    "section": "",
    "text": "I am an experienced data scientist and team leader based in New York City. I’m at my best using data to answer business questions and solve problems."
  },
  {
    "objectID": "about/me.html#experience",
    "href": "about/me.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nFlatiron School\n\nDirector, Data Science & AnalyticsMar 2022 to Present\nManager, Data Science & AnalyticsOct 2020 to Mar 2022\nSenior Data ScientistAug 2019 to Oct 2020\n\nJPMorgan Chase & Co. | Data ScientistFeb 2018 to Aug 2019\nLuxottica Retail | Senior AnalystDec 2015 to Feb 2018"
  },
  {
    "objectID": "about/me.html#education",
    "href": "about/me.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nMiami University | Oxford, OH\n\nMaster of Science in Statistics2014 - 2016\nBachelor of Science in Mathematics & Statistics2011 - 2016"
  },
  {
    "objectID": "about/me.html#skills",
    "href": "about/me.html#skills",
    "title": "About",
    "section": "Skills",
    "text": "Skills\n\n\n    \n        \n            \n        \n            \n        \n            \n        \n            \n        \n            \n        \n        \n         \n        R, SQL\n    \n\n    \n        \n            \n        \n            \n        \n            \n        \n            \n        \n        \n            \n        \n         \n        Python, Airflow, AWS: S3, Lambda, SageMaker, Glue, API Gateway, CloudWatch\n    \n\n    \n        \n            \n        \n            \n        \n            \n        \n        \n            \n        \n            \n        \n         \n        Bash, Git, Hive, Impala\n    \n\n    \n        \n            \n        \n            \n        \n        \n            \n        \n            \n        \n            \n        \n         \n        Django, JavaScript, HTML, CSS\n    \n\n\nNo matching items"
  },
  {
    "objectID": "about/me.html#hobbies",
    "href": "about/me.html#hobbies",
    "title": "About",
    "section": "Hobbies",
    "text": "Hobbies\nJiu jitsu, cooking, traveling"
  }
]